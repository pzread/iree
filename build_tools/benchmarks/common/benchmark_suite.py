# Copyright 2022 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
"""Utilities for handling the benchmark suite.

Benchmark artifacts should be generated by building the `iree-benchmark-suites`
CMake target, which put them in the following directory structure:

<root-build-dir>/benchmark_suites
└── <benchmark-category> (e.g., TFLite)
    ├── <benchmark-suite> (e.g., MobileBertSquad-fp32)
    │   ├── <benchmark-case> (e.g., iree-vulkan__GPU-Mali-Valhall__kernel-execution)
    │   │   ├── compilation_statistics.json
    │   │   ├── tool
    │   │   └── flagfile
    │   ├── ...
    │   │   ├── compilation_statistics.json
    │   │   ├── tool
    │   │   └── flagfile
    │   └── <benchmark_case>
    │   │   ├── compilation_statistics.json
    │       ├── tool
    │       └── flagfile
    └── vmfb
        ├── <compiled-iree-model>-<sha1>.vmfb
        ├── ...
        └── <compiled-iree-model>-<sha1>.vmfb
"""

from argparse import Namespace
from dataclasses import dataclass, field
import json
from typing import Optional, Sequence, Set, Tuple
from .benchmark_definition import BenchmarkResults, BenchmarkRun, DeviceInfo, BenchmarkInfo

import os
import re

# All benchmarks' relative path against root build directory.
BENCHMARK_SUITE_REL_PATH = "benchmark_suites"

# The flagfile/toolfile's filename for compiled benchmark artifacts.
MODEL_FLAGFILE_NAME = "flagfile"
MODEL_TOOLFILE_NAME = "tool"

BENCHMARK_RESULTS_REL_PATH = "benchmark-results"
CAPTURES_REL_PATH = "captures"


@dataclass
class BenchmarkConfig:
  """Represents the settings to run benchmarks.

    root_build_dir: the root build directory containing the built benchmark
      suites.
    driver_filter: filter benchmarks to those whose driver matches this regex
      (or all if this is None).
    model_name_filter: filter benchmarks to those whose model name matches this
      regex (or all if this is None).
    mode_filter: filter benchmarks to those whose benchmarking mode matches this
      regex (or all if this is None).
    tmp_dir: path to temporary directory to which intermediate outputs should be
      stored. Separate "benchmark-results" and "captures" subdirectories will be
      created as necessary.
    normal_benchmark_tool_dir: the path to the normal benchmark tool directory.
    traced_benchmark_tool_dir: the path to the tracing-enabled benchmark tool
      directory.
    trace_capture_tool: the path to the tool for collecting captured traces.
    skip_benchmarks: names of benchmarks that should be skipped. Note that
      captures will still be run for these benchmarks if do_capture is true and
      they are not also in skip_captures.
    skip_captures: names of benchmark captures that should be skipped.
    do_capture: whether captures should be collected.
    keep_going: whether to proceed if an individual run fails. Exceptions will
      logged and returned.
    benchmark_min_time: min number of seconds to run the benchmark for, if
      specified. Otherwise, the benchmark will be repeated a fixed number of
      times.
  """

  tmp_dir: str
  root_benchmark_dir: str
  benchmark_results_dir: str
  capture_dir: str
  normal_benchmark_tool_dir: Optional[str] = None
  traced_benchmark_tool_dir: Optional[str] = None
  trace_capture_tool: Optional[str] = None

  driver_filter: Optional[str] = None
  model_name_filter: Optional[str] = None
  mode_filter: Optional[str] = None

  skip_benchmarks: Set[str] = field(default_factory=set)
  skip_captures: Set[str] = field(default_factory=set)

  do_capture: bool = False
  keep_going: bool = False
  benchmark_min_time: float = 0

  @staticmethod
  def build(args: Namespace,
            tmp_dir: str,
            skip_benchmarks: Set[str] = set(),
            skip_captures: Set[str] = set()):
    """Build config from command arguments and supplementary information."""

    def real_path_or_none(path: str) -> Optional[str]:
      return os.path.realpath(path) if path else None

    if not args.normal_benchmark_tool_dir and not args.traced_benchmark_tool_dir:
      raise ValueError(
          "At least one of --normal_benchmark_tool_dir or --traced_benchmark_tool_dir should be specified."
      )

    do_capture = args.traced_benchmark_tool_dir is not None
    if ((args.traced_benchmark_tool_dir is not None) != do_capture) or (
        (args.trace_capture_tool is not None) != do_capture) or (
            (args.capture_tarball is not None) != do_capture):
      raise ValueError(
          "The following 3 flags should be simultaneously all specified or all unspecified: --traced_benchmark_tool_dir, --trace_capture_tool, --capture_tarball"
      )

    build_dir = os.path.realpath(args.build_dir)
    return BenchmarkConfig(
        tmp_dir=tmp_dir,
        root_benchmark_dir=os.path.join(build_dir, BENCHMARK_SUITE_REL_PATH),
        benchmark_results_dir=os.path.join(tmp_dir, BENCHMARK_RESULTS_REL_PATH),
        capture_dir=os.path.join(tmp_dir, CAPTURES_REL_PATH),
        normal_benchmark_tool_dir=real_path_or_none(
            args.normal_benchmark_tool_dir),
        traced_benchmark_tool_dir=real_path_or_none(
            args.traced_benchmark_tool_dir),
        trace_capture_tool=real_path_or_none(args.trace_capture_tool),
        driver_filter=args.driver_filter_regex,
        model_name_filter=args.model_name_regex,
        mode_filter=args.mode_regex,
        skip_benchmarks=skip_benchmarks,
        skip_captures=skip_captures,
        do_capture=do_capture,
        keep_going=args.keep_going,
        benchmark_min_time=args.benchmark_min_time)


@dataclass
class BenchmarkCase:
  """Represents the information to run a benchmark."""

  benchmark_info: BenchmarkInfo
  benchmark_key: str
  benchmark_case_dir: str
  normal_benchmark_tool_path: Optional[str]
  traced_benchmark_tool_path: Optional[str]
  flagfile_path: str
  benchmark_results_filename: str
  capture_filename: str
  skip_normal_benchmark: bool
  skip_traced_benchmark: bool


class BenchmarkHelper(object):
  """Helper to build benchmark cases."""

  def __init__(self, config: BenchmarkConfig, device_info: DeviceInfo):
    self.config = config
    self.device_info = device_info

  def get_available_drivers(self, verbose: bool) -> Sequence[str]:
    any_tool_dir = self.config.normal_benchmark_tool_dir if self.config.normal_benchmark_tool_dir else self.config.traced_benchmark_tool_dir
    config_txt_file_path = os.path.join(any_tool_dir, "build_config.txt")
    config_txt_file = open(config_txt_file_path, "r")
    config_txt_file_lines = config_txt_file.readlines()
    available_drivers = []
    for line in config_txt_file_lines:
      name, value = line.strip().split("=")
      if value != "ON":
        continue
      if name == "IREE_HAL_DRIVER_CUDA":
        available_drivers.append("cuda")
      elif name == "IREE_HAL_DRIVER_DYLIB":
        available_drivers.append("dylib")
      elif name == "IREE_HAL_DRIVER_DYLIB_SYNC":
        available_drivers.append("dylib-sync")
      elif name == "IREE_HAL_DRIVER_EXPERIMENTAL_ROCM":
        available_drivers.append("rocm")
      elif name == "IREE_HAL_DRIVER_VMVX":
        available_drivers.append("vmvx")
      elif name == "IREE_HAL_DRIVER_VMVX_SYNC":
        available_drivers.append("vmvx-sync")
      elif name == "IREE_HAL_DRIVER_VULKAN":
        available_drivers.append("vulkan")
      else:
        continue
    if verbose:
      available_drivers_str = ', '.join(available_drivers)
      print(f"Available drivers: {available_drivers_str}")
    return available_drivers

  def list_benchmark_categories(self) -> Sequence[str]:
    return sorted(os.listdir(self.config.root_benchmark_dir))

  def generate_benchmark_cases(
      self,
      category: str,
      cpu_target_arch: str,
      gpu_target_arch: str,
      available_drivers: Sequence[str],
      verbose: bool = False) -> Sequence[BenchmarkCase]:

    benchmark_case_dirs = self.__filter_benchmarks_for_category(
        category,
        cpu_target_arch_filter=cpu_target_arch,
        gpu_target_arch_filter=gpu_target_arch,
        available_drivers=available_drivers,
        verbose=verbose)

    benchmark_cases = []
    for benchmark_case_dir in benchmark_case_dirs:
      benchmark_case = self.__get_benchmark_case(category, benchmark_case_dir)
      if benchmark_case:
        benchmark_cases.append(benchmark_case)

    return benchmark_cases

  def __get_benchmark_case(self, category: str,
                           benchmark_case_dir: str) -> Optional[BenchmarkCase]:
    benchmark_info = self.__compose_info_object(
        category=category, benchmark_case_dir=benchmark_case_dir)
    benchmark_key = str(benchmark_info)
    skip_normal_benchmark = benchmark_key in self.config.skip_benchmarks
    skip_traced_benchmark = not self.config.do_capture or benchmark_key in self.config.skip_captures
    if skip_normal_benchmark and skip_traced_benchmark:
      return None

    normal_benchmark_tool_path = None
    traced_benchmark_tool_path = None

    # Read the file specifying which tool should be used for benchmarking
    with open(os.path.join(benchmark_case_dir, MODEL_TOOLFILE_NAME)) as f:
      tool = f.read().strip()
      if self.config.normal_benchmark_tool_dir:
        normal_benchmark_tool_path = os.path.join(
            self.config.normal_benchmark_tool_dir, tool)

      if self.config.do_capture:
        traced_benchmark_tool_path = os.path.join(
            self.config.traced_benchmark_tool_dir, tool)

    return BenchmarkCase(
        benchmark_info=benchmark_info,
        benchmark_key=benchmark_key,
        benchmark_case_dir=benchmark_case_dir,
        normal_benchmark_tool_path=normal_benchmark_tool_path,
        traced_benchmark_tool_path=traced_benchmark_tool_path,
        flagfile_path=os.path.join(benchmark_case_dir, MODEL_FLAGFILE_NAME),
        benchmark_results_filename=os.path.join(
            self.config.benchmark_results_dir, f'{benchmark_key}.json'),
        capture_filename=os.path.join(self.config.capture_dir,
                                      f'{benchmark_key}.tracy'),
        skip_normal_benchmark=skip_normal_benchmark,
        skip_traced_benchmark=skip_traced_benchmark)

  def __filter_benchmarks_for_category(self,
                                       category: str,
                                       cpu_target_arch_filter: str,
                                       gpu_target_arch_filter: str,
                                       available_drivers: Sequence[str],
                                       verbose: bool = False) -> Sequence[str]:
    """Filters benchmarks in a specific category for the given device.
    Args:
      benchmark_category_dir: the directory to a specific benchmark category.
      cpu_target_arch_filter: CPU target architecture filter regex.
      gpu_target_arch_filter: GPU target architecture filter regex.
      available_drivers: list of drivers supported by the tools in this build dir.
      verbose: whether to print additional debug info.
    Returns:
      A list containing all matched benchmark cases' directories.
    """
    matched_benchmarks = []
    benchmark_category_dir = os.path.join(self.config.root_benchmark_dir,
                                          category)
    driver_filter = self.config.driver_filter
    model_name_filter = self.config.model_name_filter
    mode_filter = self.config.mode_filter

    # Go over all benchmarks in the model directory to find those matching the
    # current Android device's CPU/GPU architecture.
    for root, _, _ in os.walk(benchmark_category_dir):
      # Take the immediate directory name and try to see if it contains compiled
      # models and flagfiles. This relies on the following directory naming
      # convention:
      #   <iree-driver>__<target-architecture>__<benchmark_mode>
      root_immediate_dir = os.path.basename(root)
      segments = root_immediate_dir.split("__")
      if len(segments) != 3 or not segments[0].startswith("iree-"):
        continue

      model_name = os.path.relpath(root, benchmark_category_dir)
      iree_driver, target_arch, bench_mode = segments
      iree_driver = iree_driver[len("iree-"):].lower()
      target_arch = target_arch.lower()

      # We can choose this benchmark if it matches the driver and CPU/GPU
      # architecture.
      matched_driver = (iree_driver in available_drivers) and (
          driver_filter is None or
          re.match(driver_filter, iree_driver) is not None)
      matched_arch = (re.match(cpu_target_arch_filter,
                               target_arch) is not None or
                      re.match(gpu_target_arch_filter, target_arch) is not None)
      matched_model_name = (model_name_filter is None or
                            re.match(model_name_filter, model_name) is not None)
      matched_mode = (mode_filter is None or
                      re.match(mode_filter, bench_mode) is not None)

      chosen = False
      if matched_driver and matched_arch and matched_model_name and matched_mode:
        matched_benchmarks.append(root)
        chosen = True

      if verbose:
        print(f"dir: {root}")
        print(f"  model_name: {model_name}")
        print(f"  iree_driver: {iree_driver}")
        print(f"  target_arch: {target_arch}")
        print(f"  bench_mode: {bench_mode}")
        print(f"  chosen: {chosen}")

    return matched_benchmarks

  def __compose_info_object(self, category: str,
                            benchmark_case_dir: str) -> BenchmarkInfo:
    """Creates an BenchmarkInfo object to describe the benchmark.
    Args:
      benchmark_category_dir: the directory to a specific benchmark category.
      benchmark_case_dir: a directory containing the benchmark case.
    Returns:
      A BenchmarkInfo object.
    """
    benchmark_category_dir = os.path.join(self.config.root_benchmark_dir,
                                          category)
    # Extract the model name from the directory path. This uses the relative
    # path under the root model directory. If there are multiple segments,
    # additional ones will be placed in parentheses.
    model_name = os.path.relpath(benchmark_case_dir, benchmark_category_dir)
    # Now we have <model-name>/.../<iree-driver>__<target-arch>__<bench_mode>,
    # Remove the last segment.
    model_name = os.path.dirname(model_name)
    main, rest = os.path.split(model_name)
    if main:
      # Tags coming from directory structure.
      model_name = main
      model_tags = [re.sub(r"\W+", "-", rest)]
    else:
      # Tags coming from the name itself.
      model_name, rest = rest.split("-", 1)
      model_tags = rest.split(",")

    # Extract benchmark info from the directory path following convention:
    #   <iree-driver>__<target-architecture>__<benchmark_mode>
    root_immediate_dir = os.path.basename(benchmark_case_dir)
    iree_driver, _, bench_mode = root_immediate_dir.split("__")

    model_source = os.path.basename(benchmark_category_dir)

    return BenchmarkInfo(model_name=model_name,
                         model_tags=model_tags,
                         model_source=model_source,
                         bench_mode=bench_mode.split(","),
                         runner=iree_driver,
                         device_info=self.device_info)


class BenchmarkDriver(object):
  """Abstract driver runs the whole benchmark flow."""

  def __init__(self,
               device_info: DeviceInfo,
               config: BenchmarkConfig,
               git_commit_hash: str,
               previous_benchmark_filenames: Set[str],
               previous_capture_filenames: Set[str],
               verbose: bool = False):
    self.device_info = device_info
    self.config = config
    self.git_commit_hash = git_commit_hash
    self.benchmark_filenames = list(previous_benchmark_filenames)
    self.capture_filenames = list(previous_capture_filenames)
    self.benchmark_errors = []
    self.verbose = verbose

  def run_benchmarks_for_category(
      self, benchmark_cases: Sequence[BenchmarkCase]
  ) -> Sequence[Tuple[Optional[str], Optional[str], Optional[Exception]]]:
    raise NotImplementedError("Should be override.")

  def run(self):
    os.makedirs(self.config.tmp_dir, exist_ok=True)

    helper = BenchmarkHelper(self.config, self.device_info)

    # Create directories on the host to store results and captures from each benchmark run.
    os.makedirs(self.config.benchmark_results_dir, exist_ok=True)
    if self.config.do_capture:
      os.makedirs(self.config.capture_dir, exist_ok=True)

    cpu_target_arch = self.device_info.get_iree_cpu_arch_name()
    gpu_target_arch = self.device_info.get_iree_gpu_arch_name()
    drivers = helper.get_available_drivers(self.verbose)

    for category in helper.list_benchmark_categories():
      benchmark_cases = helper.generate_benchmark_cases(category,
                                                        cpu_target_arch,
                                                        gpu_target_arch,
                                                        drivers, self.verbose)
      results = self.run_benchmarks_for_category(benchmark_cases)
      for (benchmark_filename, capture_filename, error) in results:
        if error:
          self.benchmark_errors.append(error)
          continue
        if benchmark_filename:
          self.benchmark_filenames.append(benchmark_filename)
        if capture_filename:
          self.capture_filenames.append(capture_filename)

  def get_benchmark_results(self):
    results = BenchmarkResults()
    results.set_commit(self.git_commit_hash)
    for b in self.benchmark_filenames:
      with open(b) as f:
        result_json_object = json.loads(f.read())
      benchmark_info = BenchmarkInfo.from_device_info_and_name(
          self.device_info,
          os.path.splitext(os.path.basename(b))[0])
      benchmark_run = BenchmarkRun(benchmark_info,
                                   result_json_object["context"],
                                   result_json_object["benchmarks"])
      results.benchmarks.append(benchmark_run)

    return results

  def get_capture_filenames(self):
    return self.capture_filenames

  def get_errors(self):
    return self.benchmark_errors

  @classmethod
  def build(cls,
            args: Namespace,
            device_info: DeviceInfo,
            git_commit_hash: str,
            verbose: bool = False):
    previous_benchmark_filenames = set()
    previous_capture_filenames = set()
    # Collect names of previous benchmarks and captures that should be skipped and
    # merged into the results.
    if args.continue_from_directory is not None:
      previous_benchmarks_dir = os.path.join(args.continue_from_directory,
                                             BENCHMARK_RESULTS_REL_PATH)
      if os.path.isdir(previous_benchmarks_dir):
        previous_benchmark_filenames = set(
            p for p in os.listdir(previous_benchmarks_dir)
            if os.path.splitext(os.path.basename(p))[1] == ".json")

      previous_captures_dir = os.path.join(args.continue_from_directory,
                                           CAPTURES_REL_PATH)
      if os.path.isdir(previous_captures_dir):
        previous_capture_filenames = set(
            p for p in os.listdir(previous_captures_dir)
            if os.path.splitext(os.path.basename(p))[1] == ".tracy")

    previous_benchmarks = set(
        os.path.splitext(os.path.basename(p))[0]
        for p in previous_benchmark_filenames)
    previous_captures = set(
        os.path.splitext(os.path.basename(p))[0]
        for p in previous_capture_filenames)

    per_commit_tmp_dir = os.path.realpath(
        os.path.join(args.tmp_dir, git_commit_hash))
    config = BenchmarkConfig.build(args=args,
                                   tmp_dir=per_commit_tmp_dir,
                                   skip_benchmarks=previous_benchmarks,
                                   skip_captures=previous_captures)

    return cls(device_info=device_info,
               config=config,
               git_commit_hash=git_commit_hash,
               previous_benchmark_filenames=previous_benchmark_filenames,
               previous_capture_filenames=previous_capture_filenames,
               verbose=verbose)
