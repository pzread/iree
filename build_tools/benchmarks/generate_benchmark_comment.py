#!/usr/bin/env python3
# Copyright 2021 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
"""Posts benchmark results to GitHub as pull request comments.

This script is meant to be used by CI. It requires the following environment
variables to be set:

- GITHUB_TOKEN: personal access token to authenticate against GitHub API;
    it should have "public_repo" and "gist" scope.
- IREE_DASHBOARD_URL: the url to IREE's performance dashboard.

This script uses pip package "markdown_strings".

Example usage:
  # Export necessary environment variables:
  export ...
  # Then run the script:
  python3 post_benchmarks_as_pr_comment.py <benchmark-json-file>...
  #   where each <benchmark-json-file> is expected to be of format expected
  #   by BenchmarkResults objects.
"""

from typing import Any, Dict, Optional, Sequence, Tuple
import argparse
import json
import markdown_strings as md
import os
import requests

from common.common_arguments import expand_and_check_file_paths
from common.benchmark_definition import execute_cmd_and_get_output
from common.benchmark_presentation import *

GITHUB_IREE_REPO_PREFIX = "https://github.com/iree-org/iree"
IREE_PROJECT_ID = 'IREE'
# The maximal numbers of trials when querying base commit benchmark results.
MAX_BASE_COMMIT_QUERY_COUNT = 10
# The max number of rows to show per table.
TABLE_SIZE_CUT = 3
THIS_DIRECTORY = pathlib.Path(__file__).resolve().parent
GIST_LINK_PLACEHORDER = "<<gist-link-placeholder>>"


@dataclass
class CommentData(object):
  pr_number: int
  type_id: str
  abbr_md: str
  full_md: str


def get_required_env_var(var: str) -> str:
  """Gets the value for a required environment variable."""
  value = os.getenv(var, None)
  if value is None:
    raise RuntimeError(f'Missing environment variable "{var}"')
  return value


def get_git_commit_hash(commit: str, verbose: bool = False) -> str:
  """Gets the commit hash for the given commit."""
  return execute_cmd_and_get_output(['git', 'rev-parse', commit],
                                    cwd=THIS_DIRECTORY,
                                    verbose=verbose)


def get_git_total_commit_count(commit: str, verbose: bool = False) -> int:
  """Gets the total commit count in history ending with the given commit."""
  count = execute_cmd_and_get_output(['git', 'rev-list', '--count', commit],
                                     cwd=THIS_DIRECTORY,
                                     verbose=verbose)
  return int(count)


def get_origin_tree_commit(base_branch: str,
                           distance: int,
                           verbose: bool = False) -> str:
  """Returns the hash for the commit with the given distance from top of the
  tree for the origin base branch."""
  # --no-recurse-submodules avoids git multiple configurations warnings on
  # submodules when the current repo is at a merged commit generated by Github
  # CI run.
  cmds = [
      'git', 'fetch', '--prune', '--no-recurse-submodules', '--', 'origin',
      base_branch
  ]
  execute_cmd_and_get_output(cmds, cwd=THIS_DIRECTORY, verbose=verbose)
  return get_git_commit_hash(f'origin/{base_branch}~{distance}', verbose)


def get_from_dashboard(url: str,
                       payload: Dict[str, Any],
                       verbose: bool = False) -> Dict[str, Dict[str, Any]]:
  headers = {'Content-type': 'application/json'}
  data = json.dumps(payload)

  if verbose:
    print(f'API request payload: {data}')

  response = requests.get(url, data=data, headers=headers)
  code = response.status_code
  if code != 200:
    raise requests.RequestException(
        f'Failed to get from dashboard server with status code {code}')

  data = response.json()
  if verbose:
    print(f'Queried base benchmark data: {data}')
  return data


def query_base_benchmark_results(
    commit: str,
    dashboard_api_url: str,
    verbose: bool = False) -> Dict[str, Dict[str, Any]]:
  """Queries the benchmark results for the given commit."""
  build_id = get_git_total_commit_count(commit, verbose)
  payload = {'projectId': IREE_PROJECT_ID, 'buildId': build_id}
  return get_from_dashboard(f'{dashboard_api_url}/getBuild',
                            payload,
                            verbose=verbose)


def get_benchmark_result_markdown(benchmark_files: Sequence[pathlib.Path],
                                  compile_stats_files: Sequence[pathlib.Path],
                                  comment_title: str,
                                  pr_commit: str,
                                  pr_url: str,
                                  build_url: str,
                                  dashboard_api_url: str,
                                  comment_type_id: str,
                                  base_branch: Optional[str] = None,
                                  verbose: bool = False) -> Tuple[str, str]:
  """Gets the full/abbreviated markdown summary of all benchmarks in files."""
  all_benchmarks = aggregate_all_benchmarks(benchmark_files,
                                            pr_commit,
                                            verbose=verbose)
  all_compilation_metrics = collect_all_compilation_metrics(
      compile_stats_files, pr_commit)

  pr_commit = md.link(pr_commit,
                      f"{GITHUB_IREE_REPO_PREFIX}/commit/{pr_commit}")

  commit_info = f"@ commit {pr_commit}"
  if base_branch is not None:
    # Collect the metric keys for each compilation target.
    compilation_metric_keys = set()
    for target_name in all_compilation_metrics:
      for mapper in COMPILATION_METRICS_TO_TABLE_MAPPERS:
        compilation_metric_keys.add(mapper.get_series_name(target_name))

    # Try to query some base benchmark to diff against, from the top of the
    # tree. Bail out if the maximal trial number is exceeded.
    for i in range(MAX_BASE_COMMIT_QUERY_COUNT):
      base_commit = get_origin_tree_commit(base_branch, i, verbose)
      base_benchmarks = query_base_benchmark_results(base_commit,
                                                     dashboard_api_url, verbose)
      base_commit = md.link(base_commit,
                            f"{GITHUB_IREE_REPO_PREFIX}/commit/{base_commit}")

      # Skip if the base doesn't contain all benchmarks to be compared.
      base_keys = set(base_benchmarks.keys())
      if (not (set(all_benchmarks.keys()) <= base_keys) or
          not (compilation_metric_keys <= base_keys)):
        commit_info = (f"@ commit {pr_commit} (no previous benchmark results to"
                       f" compare against since {base_commit})")
        continue

      # Update the aggregate benchmarks with base numbers.
      for bench in all_benchmarks:
        base_benchmark = base_benchmarks[bench]
        if base_benchmark["sampleUnit"] != "ns":
          raise ValueError("Only support nanoseconds for latency sample.")
        all_benchmarks[bench].base_mean_time = base_benchmark["sample"]

      # Update the compilation metrics with base numbers.
      for target_name, metrics in all_compilation_metrics.items():
        updated_metrics = metrics
        for mapper in COMPILATION_METRICS_TO_TABLE_MAPPERS:
          metric_key = mapper.get_series_name(target_name)
          base_benchmark = base_benchmarks[metric_key]
          if base_benchmark["sampleUnit"] != mapper.get_unit():
            raise ValueError("Unit of the queried sample is mismatched.")
          updated_metrics = mapper.update_base_value(updated_metrics,
                                                     base_benchmark["sample"])
        all_compilation_metrics[target_name] = updated_metrics

      commit_info = f"@ commit {pr_commit} (vs. base {base_commit})"
      break

  pr_info = md.link("Pull request", pr_url)
  build_info = md.link("Build", build_url)

  # Compose the full benchmark tables.
  full_table = [md.header("Full Benchmark Summary", 2)]
  full_table.append(md.unordered_list([commit_info, pr_info, build_info]))
  full_table.append(categorize_benchmarks_into_tables(all_benchmarks))

  # Compose the full compilation metrics tables.
  full_table.append(
      categorize_compilation_metrics_into_tables(all_compilation_metrics))

  # Compose the abbreviated benchmark tables.
  abbr_table = [md.header(comment_title, 2)]
  abbr_table.append(commit_info)

  abbr_benchmarks_tables = categorize_benchmarks_into_tables(
      all_benchmarks, TABLE_SIZE_CUT)
  if len(abbr_benchmarks_tables) == 0:
    abbr_table.append("No improved or regressed benchmarks ðŸ–ï¸")
  else:
    abbr_table.append(abbr_benchmarks_tables)

  abbr_compilation_metrics_tables = categorize_compilation_metrics_into_tables(
      all_compilation_metrics, TABLE_SIZE_CUT)
  if len(abbr_compilation_metrics_tables) == 0:
    abbr_table.append("No improved or regressed compilation metrics ðŸ–ï¸")
  else:
    abbr_table.append(abbr_compilation_metrics_tables)

  abbr_table.append("For more information:")
  # We don't know until a Gist is really created. Use a placeholder for now
  # and replace later.
  full_result_info = md.link("Full benchmark result tables",
                             GIST_LINK_PLACEHORDER)
  abbr_table.append(md.unordered_list([full_result_info, build_info]))

  abbr_table.append(f"<!--Comment type id: {comment_type_id}-->")

  return "\n\n".join(full_table), "\n\n".join(abbr_table)


def parse_arguments():
  """Parses command-line options."""

  parser = argparse.ArgumentParser()
  parser.add_argument(
      "--benchmark_files",
      metavar="<benchmark-json-files>",
      default=[],
      nargs="+",
      help=("Paths to the JSON files containing benchmark results, "
            "accepts wildcards"))
  parser.add_argument(
      "--compile_stats_files",
      metavar="<compile-stats-json-files>",
      default=[],
      nargs="+",
      help=("Paths to the JSON files containing compilation statistics, "
            "accepts wildcards"))
  parser.add_argument("--dry-run",
                      action="store_true",
                      help="Print the comment instead of posting to GitHub")
  parser.add_argument("--pr_commit",
                      required=True,
                      type=str,
                      help="PR commit hash")
  parser.add_argument("--pr_number", required=True, type=int, help="PR number")
  parser.add_argument("--build_url",
                      required=True,
                      type=str,
                      help="CI build page url")
  parser.add_argument("--pr_base_branch",
                      type=str,
                      default=None,
                      help="Query the dashboard for the benchmark results of "
                      "the targeting base branch")
  parser.add_argument("--comment-title",
                      default="Abbreviated Benchmark Summary",
                      help="Title of the comment")
  parser.add_argument("--comment-type-id",
                      default="f6919a4c-7bb3-4fd6-af89-97980ce49f95",
                      help="Unique id to identify previous comment")
  parser.add_argument("--output", type=pathlib.Path, default=None)
  parser.add_argument("--verbose",
                      action="store_true",
                      help="Print internal information during execution")

  args = parser.parse_args()

  return args


def main(args):
  benchmark_files = expand_and_check_file_paths(args.benchmark_files)
  compile_stats_files = expand_and_check_file_paths(args.compile_stats_files)

  dashboard_api_url = f"{get_required_env_var('IREE_DASHBOARD_URL')}/apis/v2"

  pr_number = args.pr_number
  comment_type_id = args.comment_type_id
  comment_title = args.comment_title
  full_md, abbr_md = get_benchmark_result_markdown(
      benchmark_files=benchmark_files,
      compile_stats_files=compile_stats_files,
      comment_title=comment_title,
      pr_commit=args.pr_commit,
      pr_url=f"{GITHUB_IREE_REPO_PREFIX}/pull/{pr_number}",
      build_url=args.build_url,
      dashboard_api_url=dashboard_api_url,
      comment_type_id=comment_type_id,
      base_branch=args.pr_base_branch,
      verbose=args.verbose)

  if args.dry_run:
    print(full_md, "\n\n", abbr_md)
    return

  comment_data = CommentData(pr_number=pr_number,
                             type_id=comment_type_id,
                             abbr_md=abbr_md,
                             full_md=full_md)
  comment_json_data = json.dumps(dataclasses.asdict(comment_data), indent=2)
  if args.output is not None:
    args.output.write_text(comment_json_data)

  if args.output is None or args.verbose:
    print(comment_json_data)


if __name__ == "__main__":
  main(parse_arguments())
